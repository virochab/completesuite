pipeline {
    agent any
    
    parameters {
        choice(
            name: 'TEST_TYPE',
            choices: [
                'all',
                'sanity',
                'bias',
                'aiquality',
                'rag',
                'security',
                'safety',
                'privacy',
                'agentic',
                'multiturn',
                'mcp',
                'ops',
                'unit',
                'performance'
            ],
            description: 'Select test type to run'
        )
        booleanParam(
            name: 'GENERATE_REPORTS',
            defaultValue: true,
            description: 'Generate metrics summary and evidence bundle'
        )
        booleanParam(
            name: 'ARCHIVE_ARTIFACTS',
            defaultValue: true,
            description: 'Archive test reports and evidence'
        )
    }
    
    options {
        buildDiscarder(logRotator(numToKeepStr: '30'))
        timeout(time: 4, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
    }
    
    // Scheduled triggers - uncomment to enable automatic runs
    // Daily sanity: H 2 * * * (runs daily at 2 AM)
    // Weekly comprehensive: H 3 * * 0 (runs Sunday at 3 AM)
    // 
    // Note: You can also configure these in Jenkins UI:
    // Job Configuration → Build Triggers → Build periodically
    properties([
        pipelineTriggers([
            // Uncomment the lines below to enable scheduled runs
            // cron('H 2 * * *'),  // Daily sanity at 2 AM
            // cron('H 3 * * 0')   // Weekly comprehensive on Sunday at 3 AM
        ])
    ])
    
    environment {
        PYTHON_VERSION = '3.11'
        VENV_PATH = "${WORKSPACE}/.venv"
        REPORTS_DIR = "${WORKSPACE}/reports"
        EVIDENCE_DIR = "${WORKSPACE}/evidence"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Setup Environment') {
            steps {
                script {
                    sh '''
                        echo "Setting up Python virtual environment..."
                        python${PYTHON_VERSION} -m venv ${VENV_PATH} || python3 -m venv ${VENV_PATH}
                        source ${VENV_PATH}/bin/activate || . ${VENV_PATH}/Scripts/activate
                        pip install --upgrade pip setuptools wheel
                        pip install -r requirements.txt
                        echo "Environment setup complete"
                    '''
                }
            }
        }
        
        stage('Run Tests') {
            steps {
                script {
                    // Determine test type based on parameter or default to sanity
                    def testType = params.TEST_TYPE
                    if (!testType || testType == 'null' || testType == '') {
                        // Check if this is a scheduled build
                        def causes = currentBuild.getBuildCauses('hudson.triggers.TimerTrigger$TimerTriggerCause')
                        if (causes && !causes.isEmpty()) {
                            // Scheduled build - determine type based on day
                            def currentDay = new Date().format('EEEE', TimeZone.getTimeZone('UTC'))
                            if (currentDay == 'Sunday') {
                                testType = 'all'  // Weekly comprehensive run
                                echo "Scheduled weekly comprehensive run detected"
                            } else {
                                testType = 'sanity'  // Daily sanity run
                                echo "Scheduled daily sanity run detected"
                            }
                        } else {
                            testType = 'sanity'  // Default for manual builds
                            echo "Manual build - using default sanity tests"
                        }
                    }
                    
                    def testCommand = getTestCommand(testType)
                    
                    echo "Test Type: ${testType}"
                    
                    sh """
                        source ${VENV_PATH}/bin/activate || . ${VENV_PATH}/Scripts/activate
                        echo "Running test type: ${testType}"
                        echo "Test command: ${testCommand}"
                        ${testCommand}
                    """
                }
            }
            post {
                always {
                    junit allowEmptyResults: true, testResultsPattern: 'evidence/junit-*.xml'
                }
            }
        }
        
        stage('Generate Reports') {
            when {
                expression { params.GENERATE_REPORTS == true }
            }
            steps {
                script {
                    sh """
                        source ${VENV_PATH}/bin/activate || . ${VENV_PATH}/Scripts/activate
                        echo "Generating metrics summary..."
                        python -c "
from utils.metricsHistoryTracker import MetricsHistoryTracker
tracker = MetricsHistoryTracker()
tracker.generate_summary_report()
" || echo "Warning: Could not generate metrics summary"
                        
                        echo "Building evidence bundle..."
                        python scripts/build_evidence_bundle.py || echo "Warning: Could not build evidence bundle"
                    """
                }
            }
        }
        
        stage('Archive Artifacts') {
            when {
                expression { params.ARCHIVE_ARTIFACTS == true }
            }
            steps {
                script {
                    archiveArtifacts artifacts: 'evidence/**', fingerprint: true, allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/**/*.csv', allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/**/*.json', allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/**/*.html', allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/**/*.jsonl', allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/metrics_history.csv', allowEmptyArchive: true
                    archiveArtifacts artifacts: 'reports/metrics_summary.json', allowEmptyArchive: true
                }
            }
        }
    }
    
    post {
        always {
            script {
                // Cleanup
                sh """
                    echo "Cleaning up temporary files..."
                    find . -type f -name '*.pyc' -delete || true
                    find . -type d -name '__pycache__' -exec rm -rf {} + || true
                """
            }
        }
        success {
            echo "✅ All tests passed successfully!"
        }
        failure {
            echo "❌ Some tests failed. Check the test results above."
        }
        unstable {
            echo "⚠️ Some tests were unstable or had warnings."
        }
    }
}

// Helper function to get test command based on test type
def getTestCommand(testType) {
    def commands = [:]
    
    // Sanity run - quick tests for daily validation
    commands['sanity'] = '''
        echo "Running sanity tests..."
        pytest -v -x --tb=short \
            tests/unit/ \
            tests/security/test_rate_limit.py \
            tests/security/test_authz.py \
            tests/privacy/test_pii_output.py \
            tests/safety/test_deepeval_safety_compliance.py \
            --junitxml=evidence/junit-sanity.xml \
            -m "not slow" || true
    '''
    
    // All tests - comprehensive run
    commands['all'] = '''
        echo "Running all tests..."
        pytest -v --tb=short \
            tests/ \
            --junitxml=evidence/junit-all.xml \
            --ignore=tests/performance || true
    '''
    
    // Bias tests
    commands['bias'] = '''
        echo "Running bias tests..."
        pytest -v --tb=short \
            tests/eval/test_deepeval_counterfactual_bias.py \
            --junitxml=evidence/junit-bias.xml || true
    '''
    
    // AI Quality / Behaviour tests
    commands['aiquality'] = '''
        echo "Running AI quality/behaviour tests..."
        pytest -v --tb=short \
            tests/eval/test_deepeval_behaviour.py \
            --junitxml=evidence/junit-aiquality.xml || true
    '''
    
    // RAG tests
    commands['rag'] = '''
        echo "Running RAG tests..."
        pytest -v --tb=short \
            tests/rag/test_ragas_rag.py \
            --junitxml=evidence/junit-rag.xml || true
    '''
    
    // Security tests
    commands['security'] = '''
        echo "Running security tests..."
        pytest -v --tb=short \
            tests/security/test_garak.py \
            tests/security/test_garak_tiers.py \
            tests/security/test_authz.py \
            tests/security/test_rate_limit.py \
            tests/security/test_tool_allowlist.py \
            --junitxml=evidence/junit-security.xml || true
    '''
    
    // Safety tests
    commands['safety'] = '''
        echo "Running safety tests..."
        pytest -v --tb=short \
            tests/safety/test_deepeval_safety_compliance.py \
            tests/safety/test_misuse_toxicity.py \
            tests/safety/test_non_advice_toxicity.py \
            tests/safety/test_role_violation_toxicity.py \
            tests/safety/test_pii_output.py \
            tests/safety/test_giskard_safety.py \
            tests/safety/test_rag_giskard_safety.py \
            --junitxml=evidence/junit-safety.xml || true
    '''
    
    // Privacy tests
    commands['privacy'] = '''
        echo "Running privacy tests..."
        pytest -v --tb=short \
            tests/privacy/test_pii_output.py \
            tests/privacy/test_right_to_erasure.py \
            tests/privacy/test_pii_logs_traces.py \
            tests/privacy/testPiiprompts.py \
            --junitxml=evidence/junit-privacy.xml || true
    '''
    
    // Agentic tests
    commands['agentic'] = '''
        echo "Running agentic tests..."
        pytest -v --tb=short \
            tests/agentic/ \
            --junitxml=evidence/junit-agentic.xml || true
    '''
    
    // Multiturn tests
    commands['multiturn'] = '''
        echo "Running multiturn tests..."
        pytest -v --tb=short \
            tests/multiturn/ \
            --junitxml=evidence/junit-multiturn.xml || true
    '''
    
    // MCP tests
    commands['mcp'] = '''
        echo "Running MCP tests..."
        pytest -v --tb=short \
            tests/mcp/test_deepeval_mcp.py \
            --junitxml=evidence/junit-mcp.xml || true
    '''
    
    // Operations tests
    commands['ops'] = '''
        echo "Running operations tests..."
        pytest -v --tb=short \
            tests/ops/test_fallbacks.py \
            tests/ops/test_latency_budget.py \
            --junitxml=evidence/junit-ops.xml || true
    '''
    
    // Unit tests
    commands['unit'] = '''
        echo "Running unit tests..."
        pytest -v --tb=short \
            tests/unit/ \
            --junitxml=evidence/junit-unit.xml || true
    '''
    
    // Performance tests (Locust load testing)
    commands['performance'] = '''
        echo "Running performance/load tests..."
        echo "Note: This requires the FastAPI server to be running"
        echo "Starting FastAPI server in background..."
        # Start server in background (adjust as needed)
        # python -m uvicorn app.fastapiragpdfagent:app --host 0.0.0.0 --port 8000 &
        # sleep 10
        # Run Locust tests
        # locust -f tests/performance/locustLoad.py --host=http://localhost:8000 -u 10 -r 2 --run-time 2m --headless --csv=reports/performance/locust_results --html=reports/performance/locust_report.html || true
        echo "Performance tests require manual server setup"
    '''
    
    return commands[testType] ?: commands['sanity']
}
